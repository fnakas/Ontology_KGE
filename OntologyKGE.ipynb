{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Graph Embeddings for Ontologies"
      ],
      "metadata": {
        "id": "gWNj4VwGuAT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction "
      ],
      "metadata": {
        "id": "hDzusQzauOdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab goes through a pipeline for applying KGE models on ontologies as desribed in our Medium post. It is split in four sections:\n",
        "\n",
        "\n",
        "1. **Data** Ontology to graph, graph processing, and dataset creation.\n",
        "\n",
        "2. **KGE Models** Defintions of custom KGE models.\n",
        "\n",
        "3. **Training & Evaluation** Training each KGE models on each dataset for user-defined hyper-parameters.\n",
        "\n",
        "4. **Analysis and Plotting** Plotting and analysis functions used in our experiments."
      ],
      "metadata": {
        "id": "9CJwJKcyuNhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install libraries"
      ],
      "metadata": {
        "id": "zuAB3CNw6slS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKyDalU79vJQ"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "!pip install git+https://github.com/rusty1s/pytorch_geometric.git\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install networkx\n",
        "!pip install py2neo\n",
        "!pip install rdflib\n",
        "!pip install obonet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive if your ontology folder is stored there (see below)."
      ],
      "metadata": {
        "id": "hMRnn15_6yHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6CHj_5KdrlIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "qwSibAPy4Fie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume as input ontologies in .obo format and use networkx to process and analyse them as graphs. Some of the graph processing functions we include are used in our experiments to create alternative datasets from the same ontology. As explained in the Medium post, for each ontology we experimented with three different versions using the following naming conventions (we use the \"uberon\" ontology as a running example):\n",
        "\n",
        "1) \"uberon\": we keep the graph as is\n",
        "\n",
        "2) \"uberon_trans\": we add the transitive closure of the \"is_a\" relation as a new edge type, called \"is_a_trans\" and add it back to the original graph\n",
        "\n",
        "3) \"uberon_only_trans\": graph induced by just the transitive close of \"is_a\", i.e., a graph whose only edge type is \"is_a_trans\". \n",
        "\n",
        "To construct graphs versions 2 and 3 we define functions for getting the transitive closure of an edge_type and for filtering out selected edge types. In all cases, the networkx graph representation is then transformed into a PyTorch Geometric Data object whose edges are split into train, validation, and test sets (again represented as PyG Data objects). \n"
      ],
      "metadata": {
        "id": "v9aCe4W14BPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.data import Data\n",
        "import obonet\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "# usefull graph processing functions\n",
        "\n",
        "def get_edge_counts(G):\n",
        "\n",
        "  \"\"\"\n",
        "  Get a dictionary with the edge count of each edge type\n",
        "\n",
        "  G: networkx graph with edge types\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  edge_type_counts = {}\n",
        "\n",
        "  # Iterate over the edges and their data\n",
        "  for _, _, edge_type in G.edges:\n",
        "\n",
        "      if edge_type not in edge_type_counts:\n",
        "\n",
        "          edge_type_counts[edge_type] = 0\n",
        "\n",
        "      edge_type_counts[edge_type] += 1\n",
        "\n",
        "  return edge_type_counts\n",
        "\n",
        "def add_transitive_closure(G, edge_type):\n",
        "\n",
        "  \"\"\"\n",
        "  Adds the transitive closure of edge type = edge_type in Graph G in place.\n",
        "  If the original relation type was called \"rel\" the added relation will be called \"rel_trans\"\n",
        "  Note that the original is not removed.\n",
        "\n",
        "  G: networkX graph with edge types\n",
        "  edge_type: one of the edge types\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  is_a_edges = [(u, v, edge_t) for u, v, edge_t in G.edges if edge_t == edge_type]\n",
        "  is_a_subgraph = G.edge_subgraph(is_a_edges)\n",
        "\n",
        "  is_a_transitive_closure = nx.transitive_closure(is_a_subgraph)\n",
        "\n",
        "  # Add the new edges from the transitive closure back into the original graph\n",
        "  new_is_a_edges = [(x[0], x[1], edge_type + \"_trans\") for x in set(is_a_transitive_closure.edges()) - set(is_a_subgraph.edges())]\n",
        "  G.add_edges_from(new_is_a_edges)\n",
        "\n",
        "def avg_degrees_type(G, edge_type):\n",
        "\n",
        "  \"\"\"\n",
        "  Calculate and print the average in-degree and out-degree\n",
        "  of nodes in G with respect to the sepcified edge type.\n",
        "\n",
        "  G: netowrkx graph with edge types.\n",
        "  edge_type: one of the edge types.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  in_degrees = {}\n",
        "  out_degrees = {}\n",
        "\n",
        "  for u, v, key in G.edges:\n",
        "\n",
        "      if key == edge_type:\n",
        "\n",
        "          if v not in in_degrees:\n",
        "              in_degrees[v] = 0\n",
        "          in_degrees[v] += 1\n",
        "\n",
        "          if u not in out_degrees:\n",
        "              out_degrees[u] = 0\n",
        "          out_degrees[u] += 1\n",
        "\n",
        "  avg_in_degree = sum(in_degrees.values()) / len(in_degrees)\n",
        "  avg_out_degree = sum(out_degrees.values()) / len(out_degrees)\n",
        "\n",
        "  print(\"Average in-degree for \" + edge_type + \" edge type:\", avg_in_degree)\n",
        "  print(\"Average out-degree for \" + edge_type + \" edge type:\", avg_out_degree)\n",
        "\n",
        "def obo_to_networkx (obo_path):\n",
        "\n",
        "  \"\"\"\n",
        "  Load the ontology from a local .obo file into networkx graph.\n",
        "\n",
        "  \"\"\"\n",
        "  G = obonet.read_obo(obo_path,ignore_obsolete=True)\n",
        "\n",
        "  # Convert the ontology to a NetworkX graph\n",
        "\n",
        "  G = nx.MultiDiGraph(G)\n",
        "\n",
        "  # remove degenerate edges from uberon ontology\n",
        "  \n",
        "  if obo_path.split(\"/\")[-1] == \"uberon.obo\":\n",
        "    for node in list(G.nodes()):\n",
        "        if not node.startswith(\"UBERON\"):\n",
        "            G.remove_node(node)\n",
        "\n",
        "  return G\n",
        "\n",
        "def networkx_to_pygeo (G):\n",
        "  \"\"\"\n",
        "  Transform the networkx object to a pytorch geometic Data object without loss of edge information.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # create the mapping from node labels to integer ids\n",
        "\n",
        "  node_id_map = {node_label: i for i, node_label in enumerate(G.nodes)}\n",
        "\n",
        "  # create the edge_index and edge_type arrays\n",
        "\n",
        "  edge_index = []\n",
        "  edge_type = []\n",
        "\n",
        "  for start_node, end_node, rel_type in G.edges:\n",
        "      G.edges[(start_node, end_node, rel_type)][\"edge_type\"] = rel_type\n",
        "      start_node_id = node_id_map[start_node]\n",
        "      end_node_id = node_id_map[end_node]\n",
        "      edge_index.append([start_node_id, end_node_id])\n",
        "      edge_type.append(rel_type)\n",
        "\n",
        "  # convert the edge_type array to integer ids\n",
        "\n",
        "  edge_type_map = {rel_type: i for i, rel_type in enumerate(set(edge_type))}\n",
        "  edge_type = [edge_type_map[rel_type] for rel_type in edge_type]\n",
        "\n",
        "  # create the PyTorch Geometric Data object\n",
        "\n",
        "  x = torch.zeros((len(node_id_map),)) \n",
        "  data = Data(x=x, edge_index=torch.tensor(edge_index).t().contiguous(), edge_type=torch.tensor(edge_type))\n",
        "\n",
        "  return data, node_id_map, edge_type_map\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def split_data (data, train_ratio=0.8, val_ratio=0.1):\n",
        "\n",
        "    \"\"\"\n",
        "    Edge based splitting of pytorch Data object.\n",
        "    Outputs are used for KGE training and evaluation.\n",
        "\n",
        "    Splits the data object into three new data objects \n",
        "    representing subgraphs of the original\n",
        "    with disjoint edges, but the same number of nodes.\n",
        "    Indexing is maintained across the graphs.\n",
        "\n",
        "    test_ratio is 1 - (train_ratio + val_ratio)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    num_edges = data.edge_index.shape[1]\n",
        "    edge_indices = np.arange(num_edges)\n",
        "\n",
        "    np.random.shuffle(edge_indices)\n",
        "\n",
        "    train_size = int(train_ratio * num_edges)\n",
        "    val_size = int(val_ratio * num_edges)\n",
        "\n",
        "    train_indices = edge_indices[:train_size]\n",
        "    val_indices = edge_indices[train_size:train_size + val_size]\n",
        "    test_indices = edge_indices[train_size + val_size:]\n",
        "\n",
        "    train_edge_index = data.edge_index[:, train_indices]\n",
        "    val_edge_index = data.edge_index[:, val_indices]\n",
        "    test_edge_index = data.edge_index[:, test_indices]\n",
        "\n",
        "    train_edge_types = data.edge_type[train_indices]\n",
        "    val_edge_types = data.edge_type[val_indices]\n",
        "    test_edge_types = data.edge_type[test_indices]\n",
        "\n",
        "    train_data = Data(x=data.x, edge_index=train_edge_index, edge_type = train_edge_types)\n",
        "    val_data = Data(x=data.x, edge_index=val_edge_index, edge_type = val_edge_types)  \n",
        "    test_data = Data(x=data.x, edge_index=test_edge_index, edge_type = test_edge_types)\n",
        "\n",
        "    return train_data, val_data, test_data\n"
      ],
      "metadata": {
        "id": "7gciZz9fGEiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to load the onotologies.\n",
        "\n",
        "To use: once you downloaded the ontology obo files from their respective websites (see Medium Post) put them in a folder called \"ontologies\" and add them to your workspace.\n",
        "\n",
        "Replace \"onto_path\" with your own.\n",
        "\n"
      ],
      "metadata": {
        "id": "dU2N6Ic7BeYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "onto_path = '/content/drive/MyDrive/ontologies/'\n",
        "def load_ontology_dataset (ontology, transitive_edges = [], choose_edges = False, edge_filter = []):\n",
        "  #import ontology and turn to networkx graph\n",
        "  obo_path = onto_path + ontology + '.obo'\n",
        "  G = obo_to_networkx (obo_path)\n",
        "  transitive_edge_types = transitive_edges\n",
        "  #add transitive closure of chosen edge types\n",
        "  for t in transitive_edge_types:\n",
        "    add_transitive_closure(G, t)\n",
        "  transitive_edge_types = [e + \"_trans\" for e in transitive_edge_types]\n",
        "  # choose_edges to keep\n",
        "  if choose_edges:\n",
        "    filtered_edges = [(u, v, edge_t) for u, v, edge_t in G.edges if edge_t in edge_filter]\n",
        "    G = G.edge_subgraph(filtered_edges)\n",
        "  #create pytorch geometric data object\n",
        "  data, node_dict, edge_dict = networkx_to_pygeo(G)\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  data.to(device)\n",
        "  # split edges to train, val, test\n",
        "  train_data, val_data, test_data = split_data(data,train_ratio = 0.8, val_ratio = 0.1)\n",
        "  # determine the distance types for each edge type for subsetE\n",
        "  trans_keys = [edge_dict[key] for key in transitive_edge_types]\n",
        "  dist_dict = {}\n",
        "  dist_dict[\"subsumption\"] = []\n",
        "  dist_dict[\"hausdorff\"] = []\n",
        "  for v in edge_dict.values():\n",
        "    if v in trans_keys:\n",
        "      dist_dict[\"subsumption\"].append(v)\n",
        "    else: \n",
        "      dist_dict[\"hausdorff\"].append(v)\n",
        "  \n",
        "  return {\"G\": G, \"train_data\" : train_data, \"val_data\" : val_data, \"test_data\" : test_data, \"dist_dict\" : dist_dict}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_zNwf_yQ_Rr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the ontologies and transform each to a dataset in three ways.\n",
        "\n",
        "1. as is\n",
        "2. add transitive closuse of \"is_a\" relation\n",
        "3. include only the transitive closure\n",
        "\n",
        "Combine all the outputs in a dictionary labeled by the ontology name (see name conventions above). Each value is another dictionary containg the train,val, test objects as well as some stats and the \"dist_dict\" dictionary used to tell subsetE what distance function it should use to calculate the loss for each relation type (see the implementation of the score function in the definition of SubsetE in the KGE models section)."
      ],
      "metadata": {
        "id": "jy6ZBfvLA0xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ontologies = [\"cl\", \"go\", \"hp\", \"uberon\"]\n",
        "onto_datasets= {}\n",
        "for o in ontologies:\n",
        "  \n",
        "  #add as is\n",
        "  ont_dict = load_ontology_dataset(o)\n",
        "  G = ont_dict[\"G\"]\n",
        "  ont_dict[\"num_nodes\"] = G.number_of_nodes()\n",
        "  ont_dict[\"num_edges\"] = G.number_of_edges()\n",
        "  ont_dict[\"edges_counts\"] = get_edge_counts(G)\n",
        "  onto_datasets[o] = ont_dict\n",
        "  # add with trans closure\n",
        "  o_trans = o + \"_trans\"\n",
        "  ont_dict = load_ontology_dataset(o, transitive_edges=[\"is_a\"])\n",
        "  G = ont_dict[\"G\"]\n",
        "  ont_dict[\"num_nodes\"] = G.number_of_nodes()\n",
        "  ont_dict[\"num_edges\"] = G.number_of_edges()\n",
        "  ont_dict[\"edges_counts\"] = get_edge_counts(G)\n",
        "  onto_datasets[o_trans] = ont_dict\n",
        "  # add only with trans closure\n",
        "  o_trans = o + \"_only_trans\"\n",
        "  ont_dict = load_ontology_dataset(o, transitive_edges=[\"is_a\"],choose_edges=True, edge_filter=[\"is_a_trans\"])\n",
        "  G = ont_dict[\"G\"]\n",
        "  ont_dict[\"num_nodes\"] = G.number_of_nodes()\n",
        "  ont_dict[\"num_edges\"] = G.number_of_edges()\n",
        "  ont_dict[\"edges_counts\"] = get_edge_counts(G)\n",
        "  onto_datasets[o_trans] = ont_dict\n"
      ],
      "metadata": {
        "id": "KyAgCMZt_Syb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KGE Models"
      ],
      "metadata": {
        "id": "pJKqvplk5CNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PyG representation allows us to apply the KGE functionalities of the PyG library which center around the abstract KGEModel class (https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.KGEModel.html). This class already has TransE, ComplexE, and DistMult as instances but also allows one to easily create custom KGE models. We illustrate this with two examples. \n",
        "\n",
        "The first one is a loose implementation of TransR, where we have assumed for simplity that the node and relation embeddings have the same dimension (so the rel_W matrix of each relation is square). \n",
        "\n",
        "The second one is SubsetE, an original method we developed with the aim of leveraging the class hierarchy of nodes in ontology graphs based on the \"is a\" relation (for example: dog - is a -> mammal). Roughly, the idea is to represent node embeddings (i.e. ontology concepts) as sets of points (vs single points) in H-dimensional space delimiting a region between them which respesents the \"range\" of the said concept. We then use a loss function that forces children of the is a relation to be repsented by regions that are contained in the region of their parent concepts (so the embedding region of \"dog\" should be a subset of the embedding region of \"pet\" which should in turn be contained in the region of \"mammal\"). For reasons explained in detail in the blog post this method proved ineffective largely due to the difficulty of balancing opposing loss objectives without making the training unstable. However, for future work and experimentation we include our working implementation below, along with all the distance functions we experimented with to construct loss functions."
      ],
      "metadata": {
        "id": "IciRxLCc4irj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our implementation of TransR\n",
        "\n",
        "Restriction: for ease of use and since we are only interested in \"big_picture\" qualitative results we assume that the embedding and relation space have the same dimension i.e. \"hidden_channels\". \n"
      ],
      "metadata": {
        "id": "hpKnGTaJAd-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding\n",
        "from torch_geometric.nn.kge import KGEModel\n",
        "\n",
        "\n",
        "class TransR(KGEModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        num_relations: int,\n",
        "        hidden_channels: int,\n",
        "        margin: float = 1.0,\n",
        "        p_norm: float = 1.0,\n",
        "        sparse: bool = False,\n",
        "    ):\n",
        "        self.p_norm = p_norm\n",
        "        self.margin = margin\n",
        "\n",
        "        super().__init__(num_nodes, num_relations, hidden_channels, sparse)\n",
        "\n",
        "        self.node_emb = Embedding(num_nodes, hidden_channels, sparse=sparse)\n",
        "        self.rel_emb = Embedding(num_relations, hidden_channels, sparse=sparse)\n",
        "        \n",
        "        self.rel_W = Embedding(num_relations, hidden_channels * hidden_channels, sparse=sparse)\n",
        "        \n",
        "      \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        bound = 6. / math.sqrt(self.hidden_channels)\n",
        "        torch.nn.init.uniform_(self.node_emb.weight, -bound, bound)\n",
        "        torch.nn.init.uniform_(self.rel_emb.weight, -bound, bound)\n",
        "        F.normalize(self.rel_emb.weight.data, p=self.p_norm, dim=-1,\n",
        "                    out=self.rel_emb.weight.data)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        head_index: Tensor,\n",
        "        rel_type: Tensor,\n",
        "        tail_index: Tensor,\n",
        "    ) -> Tensor:\n",
        "\n",
        "        head = self.node_emb(head_index)\n",
        "        tail = self.node_emb(tail_index)\n",
        "        rel = self.rel_emb(rel_type)\n",
        "        rel_W = self.rel_W(rel_type)\n",
        "        rel_W = rel_W.view(-1, self.hidden_channels, self.hidden_channels) \n",
        "        rel_W = rel_W[rel_type]\n",
        "\n",
        "        # Perform batched matrix multiplication\n",
        "\n",
        "        head = F.normalize(head, p=self.p_norm, dim=-1)\n",
        "        tail = F.normalize(tail, p=self.p_norm, dim=-1)\n",
        "\n",
        "        head = torch.bmm(rel_W, head.unsqueeze(-1)).squeeze(-1)\n",
        "        tail = torch.bmm(rel_W, tail.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "        head = F.normalize(head, p=self.p_norm, dim=-1)\n",
        "        tail = F.normalize(tail, p=self.p_norm, dim=-1)\n",
        "\n",
        "\n",
        "        # Calculate *negative* TransR norm:\n",
        "        return -((head + rel) - tail).norm(p=self.p_norm, dim=-1)\n",
        "\n",
        "    def loss(\n",
        "        self,\n",
        "        head_index: Tensor,\n",
        "        rel_type: Tensor,\n",
        "        tail_index: Tensor,\n",
        "    ) -> Tensor:\n",
        "\n",
        "        pos_score = self(head_index, rel_type, tail_index)\n",
        "        neg_score = self(*self.random_sample(head_index, rel_type, tail_index))\n",
        "\n",
        "        return F.margin_ranking_loss(\n",
        "            pos_score,\n",
        "            neg_score,\n",
        "            target=torch.ones_like(pos_score),\n",
        "            margin=self.margin,\n",
        "        )"
      ],
      "metadata": {
        "id": "OcKiEbc_HX9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility, distance, and normalization/potential functions for defining SubsetE. Note that not all of them are used in the defintion that comes after. See the function descriptions and the Medium post for the role of each."
      ],
      "metadata": {
        "id": "jDpY0x_A_8iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions for subsetE\n",
        "\n",
        "\n",
        "def can_broadcast(s, t):\n",
        "    \"\"\"Given tuples/lists s and t, returns iff tensors of size s and t can be broadcasted,\n",
        "       i.e. len(s) = len(t) and s[i] == t[i] unless s[i] == 1 or t[i] == 1.\n",
        "    \"\"\"\n",
        "    if len(s) != len(t):\n",
        "        return False\n",
        "    for i in range(len(s)):\n",
        "        if s[i] != 1 and t[i] != 1 and s[i] != t[i]:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def gravity_distance(A,B):\n",
        "\n",
        "\n",
        "  center_B = B.mean(dim=1)\n",
        "\n",
        "  # detach the center of the parent node so that it does not get destabilized\n",
        "  center_B = center_B.detach()\n",
        "\n",
        "  # Calculate the Euclidean distances between all points in A and the center of B\n",
        "  squared_diffs = (A - center_B.unsqueeze(1))**2\n",
        "  squared_distances = squared_diffs.sum(dim=-1)\n",
        "  distances = torch.sqrt(squared_distances)\n",
        "\n",
        "  # Calculate the average distance for each set of points in A from the center of B\n",
        "  average_distances = distances.mean(dim=-1)\n",
        "\n",
        "  return average_distances\n",
        "\n",
        "def min_distance(x, y, p = 2):\n",
        "  \"\"\"\n",
        "  Given two finite subsets x,y of R^d, returns their L^p distance:\n",
        "  Supposing that x = {x_1, ... x_m}\n",
        "  and y = {y_1, ... y_n}, where x_i,y_j are in R^d, we have \n",
        "  min_distance(x,y) = min_{i,j} ||x_i-y_j||_p.\n",
        "\n",
        "  x and y are tensors of shape (S, *, d), where * is the size of the \n",
        "  appropriate sets, and where S is an arbitrary tuple (parallelizing the computation).\n",
        "  p is the norm we are using.\n",
        "\n",
        "  Returns a tensor of shape S.\n",
        "  \"\"\"\n",
        "  assert(can_broadcast(x.size()[:-2], y.size()[:-2]))\n",
        "  assert(x.size()[-1] == y.size()[-1])\n",
        "\n",
        "  x_expand = x[..., None, :, :]\n",
        "  y_expand = y[..., :, None, :]\n",
        "  pairwise_dists = torch.norm(x_expand - y_expand, p, dim = -1)\n",
        "  min_dist = torch.amin(pairwise_dists, dim = [-2,-1])\n",
        "\n",
        "  return min_dist\n",
        "\n",
        "def subsumption_distance(x, y, p = None):\n",
        "  \"\"\"\n",
        "  Given two finite subsets x,y of R^d, returns their subsumption distance, \n",
        "  using distance function dist. Supposing that x = {x_1, ... x_m}\n",
        "  and y = {y_1, ... y_n}, where x_i,y_j are in R^d, we have \n",
        "  set_distance(x,y) = max_{i} set_distance({x_i}, y)\n",
        "\n",
        "  x and y are tensors of shape (S, *, d), where * is the size of the \n",
        "  appropriate sets, and where S is arbitrary (parallelizing the computation).\n",
        "  p is the norm we are using. Supports array broadcasting.\n",
        "  \"\"\"\n",
        "\n",
        "  assert(can_broadcast(x.size()[:-2], y.size()[:-2]))\n",
        "  assert(x.size()[-1] == y.size()[-1])\n",
        "\n",
        "  x_expand = x[..., :, None, :]\n",
        "  y_expand = y[..., None, :, :]\n",
        "  \n",
        "  #compute set distance of each point of x to all of y\n",
        "  x_point_dists = min_distance(x_expand, y_expand, p)\n",
        "\n",
        "  #take maximum of above distances\n",
        "  subsumption_dist = torch.amax(x_point_dists, dim = -1)\n",
        "\n",
        "  return subsumption_dist\n",
        "\n",
        "def hausdorff_distance(x, y, p = 2):\n",
        "  \"\"\"\n",
        "  Given two finite subsets x,y of R^d, returns their Hausdorff distance, \n",
        "  using distance function dist. Supposing that x = {x_1, ... x_m}\n",
        "  and y = {y_1, ... y_n}, where x_i,y_j are in R^d, we have \n",
        "  set_distance(x,y) = max(subsumption_distance(x,y), subsumption_distance(y,x))\n",
        "\n",
        "  x and y are tensors of shape (S, *, d), where * is the size of the \n",
        "  appropriate sets, and where S is arbitrary (parallelizing the computation).\n",
        "  p is the norm we are using. Supports array broadcasting.\n",
        "  \"\"\"\n",
        "\n",
        "  assert(can_broadcast(x.size()[:-2], y.size()[:-2]))\n",
        "  assert(x.size()[-1] == y.size()[-1])\n",
        "\n",
        "  return torch.maximum(subsumption_distance(x,y), subsumption_distance(y,x))\n",
        "\n",
        "def set_distance(x, y, dist_type = \"hausdorff\", p = 2):\n",
        "\n",
        "    \"\"\"\n",
        "    Given two finite subsets x,y of R^d, returns their distance.\n",
        "    The type of distance is selected in dist_type. The norm used \n",
        "    for calculations is the Lp norm. THis is just another way to call\n",
        "    the above distance functions.\n",
        "\n",
        "    x and y are tensors of shape (S, *, d), where * is the size of the \n",
        "    appropriate sets, and where S is arbitrary (parallelizing the computation).\n",
        "    p is the norm we are using. Supports array broadcasting.\n",
        "    \"\"\"\n",
        "\n",
        "    if dist_type == \"minimum\":\n",
        "        return min_distance(x, y, p = 2)\n",
        "\n",
        "    elif dist_type == \"subsumption\":\n",
        "        return subsumption_distance(x, y, p = 2)\n",
        "\n",
        "    elif dist_type == \"hausdorff\":\n",
        "        return hausdorff_distance(x, y, p = 2)\n",
        "    elif dist_type == \"gravity\":\n",
        "        return gravity_distance(x,y)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\"Not a valid set distance type!\")\n",
        "\n",
        "def potential(x, p = 2, pow = 2, C = 1/2):\n",
        "    \"\"\"\n",
        "    Given a finite subset x of R^d, returns a potential\n",
        "    which is high if the points are far apart from one another,\n",
        "    and is 0 iff the points are all the same.\n",
        "\n",
        "    Input: Tensor x of shape (S, k, d). k is the size of the subset\n",
        "    and d is the dimension of the embedding space. S is arbitrary,\n",
        "    and allows this function to be computed in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    x_i = x[..., :, None, :]\n",
        "    x_j = x[..., None, :, :]\n",
        "\n",
        "    dists = torch.norm(x_i - x_j, p = p, dim = -1)\n",
        "\n",
        "    powdists = torch.pow(dists, pow)\n",
        "\n",
        "    #set diagonal elements to 0 \n",
        "    # diag = torch.diagonal(powdists, dim1 = -2, dim2 = -1)\n",
        "    # diag[True] = 0\n",
        "\n",
        "    sumpowdists = torch.sum(powdists, dim = (-2,-1))\n",
        "\n",
        "    return 1/2 * C * sumpowdists\n",
        "\n",
        "def sum_of_sqs_potential(x):\n",
        "    \"\"\"\n",
        "    Given a finite subset x of R^d, returns the sum of squares potential.\n",
        "\n",
        "    Input: Tensor x of shape (S, k, d). k is the size of the subset\n",
        "    and d is the dimension of the embedding space. S is arbitrary,\n",
        "    and allows this function to be computed in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    return potential(x, p = 2, pow = 2, C = 1/2)\n",
        "\n",
        "def newton_potential(x):\n",
        "    \"\"\"\n",
        "    Given a finite subset x of R^d, returns (un-normalized) newton potential.\n",
        "    Asserts that d >= 3 to avoid computing log.\n",
        "\n",
        "    Input: Tensor x of shape (S, k, d). k is the size of the subset\n",
        "    and d is the dimension of the embedding space. S is arbitrary,\n",
        "    and allows this function to be computed in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    emb_dim = x.shape[-1] #dimension of embeddings\n",
        "\n",
        "    assert emb_dim >= 3, \"embedding dimension must be at least 3 for newton potential\"\n",
        "\n",
        "    return potential(x, p = 2, pow = 2 - emb_dim, C = -1)\n",
        "\n",
        "\n",
        "def expand_last_dimension(x, last_dim):\n",
        "    \"\"\"\n",
        "    Given a Tensor of shape (S, d), where S is an arbitrary tuple. Asserts that\n",
        "    last_dim divides d. Returns x reshaped into a Tensor of shape\n",
        "    (S, d/last_dim, last_dim). \n",
        "\n",
        "    Example: if S = (), d = 4, last_dim = 2, \n",
        "    and x = [1, 2, 3, 4], returns [[1, 2],\n",
        "                                    3, 4]]\n",
        "    \n",
        "    \"\"\"\n",
        "    S = x.shape[:-1]\n",
        "    d = x.shape[-1]\n",
        "    assert d % last_dim == 0, \"invalid operation, last dimension can not be expanded into this shape\"\n",
        "\n",
        "    return torch.reshape(x, S + (d//last_dim, last_dim))\n",
        "\n",
        "def barrier_potential(x, p = 2):\n",
        "    \"\"\"\n",
        "    Given a finite subset x of R^d, returns a potential\n",
        "    which incentivizes points to lie in the unit Lp  ball\n",
        "\n",
        "    Input: Tensor x of shape (S, k, d). k is the size of the subset\n",
        "    and d is the dimension of the embedding space. S is arbitrary,\n",
        "    and allows this function to be computed in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    R = torch.nn.ReLU()\n",
        "    return torch.sum(R(torch.norm(x, dim = -1, p = 2) - 1), dim = -1)\n",
        "\n"
      ],
      "metadata": {
        "id": "ynbVZ5XPbT7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SubsetE defintion using the KGE model class. The comments provide some guidance as to functionality but for the big picture refer to the mathematical defintions of the Medium Post."
      ],
      "metadata": {
        "id": "oSFofVI2CQsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "from torch_geometric.nn.kge import KGEModel\n",
        "\n",
        "class SubsetE(KGEModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        #copied from TransE. Can remove parameters if not needed.\n",
        "        num_nodes: int,\n",
        "        num_relations: int,\n",
        "        emb_dim: int, #embedding dimension\n",
        "        dist_types: dict,\n",
        "        margin: float = 1.0,\n",
        "        p_norm: float = 1.0,\n",
        "        pts_per_node: int = 5, #number of embedding points per node\n",
        "        sparse: bool = False,\n",
        "    ):\n",
        "        self.emb_dim = emb_dim\n",
        "        self.p_norm = p_norm\n",
        "        self.margin = margin\n",
        "        self.dist_types = dist_types\n",
        "        self.pts_per_node = pts_per_node\n",
        "        super().__init__(num_nodes, num_relations, pts_per_node * emb_dim, sparse)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        bound = 6. / math.sqrt(self.hidden_channels)\n",
        "        torch.nn.init.uniform_(self.node_emb.weight, -bound, bound)\n",
        "        torch.nn.init.uniform_(self.rel_emb.weight, -bound, bound)\n",
        "        F.normalize(self.rel_emb.weight.data, p=self.p_norm, dim=-1,\n",
        "                    out=self.rel_emb.weight.data)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        head_index: Tensor,\n",
        "        rel_type: Tensor,\n",
        "        tail_index: Tensor,\n",
        "    ) -> Tensor:\n",
        "\n",
        "        #split up index based on node classes, and recursively\n",
        "        #call forward for each node class. Then concatenate the results.\n",
        "\n",
        "        outs = []\n",
        "\n",
        "        # compute sliced head_index, rel_type, and tail_index by dist_type\n",
        "        for dist_type, edge_types in self.dist_types.items():\n",
        "\n",
        "            #check which triples have an edge type with the current dist type\n",
        "\n",
        "            mask = torch.zeros_like(rel_type, dtype=torch.bool)\n",
        "            for edge_type in edge_types:\n",
        "                mask = torch.logical_or(mask, rel_type == edge_type)\n",
        "\n",
        "            # Slice the input tensors using the boolean mask\n",
        "\n",
        "            head_index_slice = head_index[mask]\n",
        "            rel_type_slice = rel_type[mask]\n",
        "            tail_index_slice = tail_index[mask]\n",
        "\n",
        "            # Add scores of current edge type to total scores list\n",
        "\n",
        "            outs.append(self.score(head_index_slice,\n",
        "                                   rel_type_slice,\n",
        "                                   tail_index_slice,\n",
        "                                   dist_type))\n",
        "          \n",
        "        return torch.cat(outs, dim = 0) \n",
        "\n",
        "\n",
        "    def score(\n",
        "        self,\n",
        "        head_index: Tensor,\n",
        "        rel_type: Tensor,\n",
        "        tail_index: Tensor,\n",
        "        dist_type\n",
        "    ) -> Tensor:\n",
        "\n",
        "        #assert dist_type in self.dist_types.keys()\n",
        "\n",
        "        head = self.node_emb(head_index)\n",
        "        rel = self.rel_emb(rel_type)\n",
        "        tail = self.node_emb(tail_index)\n",
        "\n",
        "        #reshape trans_head, tail into a hidden_channels * emb_per_node matrix \n",
        "        head = expand_last_dimension(head, self.emb_dim)\n",
        "        tail = expand_last_dimension(tail, self.emb_dim)\n",
        "        rel = expand_last_dimension(rel, self.emb_dim)\n",
        "\n",
        "        #head = F.normalize(head, p=self.p_norm, dim=-1) #Question.. whether to normalize or not? or include another potential term?\n",
        "        #tail = F.normalize(tail, p=self.p_norm, dim=-1)\n",
        "\n",
        "        translated_head = head #+ rel\n",
        "\n",
        "        return -1 * set_distance(head, tail, dist_type, p = self.p_norm)\n",
        "\n",
        "\n",
        "    def loss(\n",
        "        self,\n",
        "        head_index: Tensor,\n",
        "        rel_type: Tensor,\n",
        "        tail_index: Tensor,\n",
        "        print_bool = False\n",
        "    ) -> Tensor:\n",
        "\n",
        "        #calcualate negative and positive scores to compute margin loss (like other KGE methods)\n",
        "        #note that unlike the positiive score distance function which depends on the edge_type\n",
        "        #the negative score loss uses the same distance function for all kinds of negative edges.\n",
        "\n",
        "        pos_score = self(head_index, rel_type, tail_index)\n",
        "        neg_head_index, neg_rel_type, neg_tail_index = self.random_sample(head_index, rel_type, tail_index)\n",
        "        neg_score = self.score(neg_head_index, neg_rel_type, neg_tail_index, dist_type = \"subsumption\")\n",
        "\n",
        "\n",
        "        marg_loss = F.margin_ranking_loss(\n",
        "            pos_score,\n",
        "            neg_score,\n",
        "            target=torch.ones_like(pos_score),\n",
        "            margin=self.margin,\n",
        "        )\n",
        "        \n",
        "\n",
        "        #get the embeddings to calculate ReLu \"normalization\" loss \n",
        "        #(also refered to as gravity as it pulls embeddigns toward the unit circle, see barrier_potential definition)\n",
        "        head = self.node_emb(head_index)\n",
        "        tail = self.node_emb(tail_index)\n",
        "        exp_head = expand_last_dimension(head, self.emb_dim)\n",
        "        exp_tail = expand_last_dimension(head, self.emb_dim)\n",
        "        norm_loss = 100*(torch.sum(barrier_potential(exp_head)) + torch.sum(barrier_potential(exp_tail)))\n",
        "\n",
        "        # calculate potential loss: penalizes an embedding when its points lie to close to each other\n",
        "        # meant to counter the tendency of the embeddings to colapse to points\n",
        "        potential_loss = torch.sum(sum_of_sqs_potential(exp_head)) + torch.sum(gravity_distance(exp_head, exp_head))\n",
        "        potential_loss += torch.sum(sum_of_sqs_potential(exp_tail)) + torch.sum(gravity_distance(exp_tail, exp_tail))\n",
        "\n",
        "\n",
        "        # combine losses\n",
        "\n",
        "        loss = norm_loss + marg_loss + potential_loss/(100 * (self.pts_per_node)^2) \n",
        "\n",
        "        return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "71vu6rBDOaUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Evaluation"
      ],
      "metadata": {
        "id": "mmokxz8R6Iou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closely based on the PyG team's example training code for KGEs (https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/kge/transe.html#TransE)\n",
        "\n",
        "Having processed our ontologies and defined/chosen our KGE models we proceed to training each of the later on each the former, for user-specifiable hyperparameter ranges. In our own experiments, having limited time and GPU credits, we hard coded all hyperparameters based on the values used in the PyG example for training KGE models but our code directly supports hyperparameter tuning. To enable it, please replace the hard coded values in the code with the names of the loop iterators.\n",
        "\n",
        "param_combinations_outer: should be created as the product of all parameter option lists that are not optimization over in the validation stage\n",
        "\n",
        "param_combinations_inner: should be as the product of all parameter option lists that are optimized over in the validation stage. To incude the embedding size as a hyperparameter move it from outer to inner.\n"
      ],
      "metadata": {
        "id": "WfEJZO7t6HNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import ComplEx, TransE, DistMult\n",
        "import itertools\n",
        "import copy\n",
        "\n",
        "\n",
        "model_map = {'transE': TransE, 'transR' : TransR, 'complexE': ComplEx, \"distM\": DistMult}\n",
        "\n",
        "# models and datasets and embedding size to loop over \n",
        "#(note that transR will receive more parameters than the other models for the same hidden channels)\n",
        "models = list(model_map.keys())\n",
        "datasets = list(onto_datasets.keys())\n",
        "hidden_channels = [50]\n",
        "\n",
        "# hyperparameters for fine-tuning\n",
        "# although we hard hard coded them using the ones from the examples of the PyG team \n",
        "# hyperparameter tuning is fully functional in this script.\n",
        "# Just replace the hard coded values with the variables \"lr\" and \"batch\" size of the inner loop.\n",
        "# You could also move the hidden channels from outer to the inner loop to optimize over\n",
        "\n",
        "lr_rates = [0.01]\n",
        "batch_sizes = [1000]\n",
        "results = []\n",
        "\n",
        "param_combinations_outer = itertools.product(models, datasets, hidden_channels)\n",
        "\n",
        "results = []\n",
        "\n",
        "for my_model, my_dataset, hidden_channels in param_combinations_outer:\n",
        "\n",
        "    print(\"model\")\n",
        "    print(my_model)\n",
        "    print(\"dataset\")\n",
        "    print(my_dataset)\n",
        "    print(\"hidden_channels\")\n",
        "    print(hidden_channels)\n",
        "\n",
        "    \n",
        "\n",
        "    best_mean_rank = float('inf')\n",
        "    best_hits_at_10 = 0.0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    param_combinations_inner = itertools.product(lr_rates, batch_sizes)\n",
        "\n",
        "    for lr, batch_size in param_combinations_inner:\n",
        "\n",
        "        print(\"lr\")\n",
        "        print(lr)\n",
        "        print(\"batch_size\")\n",
        "        print(batch_size)\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        train_data = onto_datasets[my_dataset][\"train_data\"]\n",
        "        val_data = onto_datasets[my_dataset][\"val_data\"]\n",
        "        test_data = onto_datasets[my_dataset][\"test_data\"]\n",
        "\n",
        "\n",
        "        model = model_map[my_model](\n",
        "        num_nodes= train_data.num_nodes,\n",
        "        num_relations= train_data.num_edge_types,\n",
        "        hidden_channels=hidden_channels,\n",
        "        sparse = False\n",
        "        ).to(device)\n",
        "\n",
        "        loader = model.loader(\n",
        "            head_index=train_data.edge_index[0],\n",
        "            rel_type=train_data.edge_type,\n",
        "            tail_index=train_data.edge_index[1],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "        optimizer_map = {\n",
        "            'transE': optim.Adam(model.parameters(), lr=0.01),\n",
        "            'transR': optim.Adam(model.parameters(), lr=0.01),\n",
        "            'complexE': optim.Adagrad(model.parameters(), lr=0.001, weight_decay=1e-6),\n",
        "            'distM': optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6),\n",
        "        }\n",
        "        optimizer = optimizer_map[my_model]\n",
        "\n",
        "\n",
        "        def train():\n",
        "            model.train()\n",
        "            total_loss = total_examples = 0\n",
        "            for head_index, rel_type, tail_index in loader:\n",
        "                optimizer.zero_grad()\n",
        "                loss = model.loss(head_index, rel_type, tail_index)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += float(loss) * head_index.numel()\n",
        "                total_examples += head_index.numel()\n",
        "            return total_loss / total_examples\n",
        "\n",
        "\n",
        "        #inner loop evalaution on validation set\n",
        "        @torch.no_grad()\n",
        "        def test(data):\n",
        "            model.eval()\n",
        "            return model.test(\n",
        "                head_index=data.edge_index[0],\n",
        "                rel_type=data.edge_type,\n",
        "                tail_index=data.edge_index[1],\n",
        "                batch_size=20000,\n",
        "                k=10,\n",
        "            )\n",
        "\n",
        "\n",
        "        for epoch in range(1, 501):\n",
        "            loss = train()\n",
        "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
        "            if epoch % 250 == 0:\n",
        "                mean_rank, hits = test(val_data)\n",
        "                print(f'Epoch: {epoch:03d}, Val Mean Rank: {mean_rank:.2f}, '\n",
        "                      f'Val Hits@10: {hits:.4f}')\n",
        "\n",
        "        if mean_rank < best_mean_rank:\n",
        "            best_mean_rank = mean_rank\n",
        "            best_hits_at_10 = hits\n",
        "            best_params = (lr, batch_size)\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "    #evaluate best model on test set\n",
        "    @torch.no_grad()\n",
        "    def test(data):\n",
        "        best_model.eval()\n",
        "        return best_model.test(\n",
        "            head_index=data.edge_index[0],\n",
        "            rel_type=data.edge_type,\n",
        "            tail_index=data.edge_index[1],\n",
        "            batch_size=20000,\n",
        "            k=10,\n",
        "        )\n",
        "\n",
        "    test_mean_rank, test_hits_at_10 = test(test_data)\n",
        "    print(f'Test Mean Rank: {test_mean_rank:.2f}, Test Hits@10: {test_hits_at_10:.4f}')\n",
        "\n",
        "    result = {\n",
        "    'model': my_model,\n",
        "    'dataset': my_dataset,\n",
        "    \"hidden_dims\": hidden_channels,\n",
        "    \"loss\": loss,\n",
        "    'val_mean_rank': best_mean_rank,\n",
        "    'val_hits_at_10': best_hits_at_10,\n",
        "    'best_params': best_params,\n",
        "    'test_mean_rank': test_mean_rank,\n",
        "    'test_hits_at_10': test_hits_at_10,\n",
        "    }\n",
        "    results.append(result)\n"
      ],
      "metadata": {
        "id": "mJCmukz7zUGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "N_sE7Ez46gSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We include here all analysis and plotting functions we used for data exploration, analysis, and subsetE experimentation that are not crucial to running the ontology KGE pipeline. "
      ],
      "metadata": {
        "id": "Vg35RlMSFbx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count ontology edge counts for unprocessed ontology graphs and plot histogram."
      ],
      "metadata": {
        "id": "M1kq_bYWGCc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot edge counts\n",
        "\n",
        "counts = {}\n",
        "for o in ontologies:\n",
        "  c = {}\n",
        "  edge_counts = onto_datasets[o][\"edges_counts\"]\n",
        "  c[\"is_a\"] = edge_counts[\"is_a\"]\n",
        "  total_edges = onto_datasets[o][\"num_edges\"]\n",
        "  c[\"others\"] = total_edges - c[\"is_a\"] \n",
        "  counts[o] = c\n",
        "\n",
        "df = pd.DataFrame(counts)\n",
        "df_t = df.T\n",
        "\n",
        "# Plot the histogram\n",
        "ax = df_t.plot(kind='bar', stacked=False)\n",
        "plt.title(\"Edge Types Counts per Ontology\")\n",
        "plt.xlabel(\"Ontologies\")\n",
        "plt.ylabel(\"Edges\")\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tLmbTHuQlLez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2961fc5f-c622-4d34-ed12-ece8187ee5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count ontology edge counts for ontology graph versions that include the \"is_a_trans\" edge type and plot histogram."
      ],
      "metadata": {
        "id": "AbdNRTOnH2Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot edge counts\n",
        "\n",
        "counts = {}\n",
        "for o in [x + \"_trans\" for x in ontologies]:\n",
        "  c = {}\n",
        "  edge_counts = onto_datasets[o][\"edges_counts\"]\n",
        "  c[\"is_a\"] = edge_counts[\"is_a\"]\n",
        "  c[\"is_a_trans\"] = edge_counts[\"is_a_trans\"]\n",
        "  total_edges = onto_datasets[o][\"num_edges\"]\n",
        "  c[\"others\"] = total_edges - (c[\"is_a\"] + c[\"is_a_trans\"])\n",
        "  counts[o] = c\n",
        "\n",
        "df = pd.DataFrame(counts)\n",
        "\n",
        "\n",
        "# Transpose the DataFrame for easier plotting\n",
        "df_t = df.T\n",
        "\n",
        "# Plot the histogram\n",
        "ax = df_t.plot(kind='bar', stacked=False)\n",
        "plt.title(\"Edge Types Counts per Ontology with Transitive 'is_a'\")\n",
        "plt.xlabel(\"Ontologies\")\n",
        "plt.ylabel(\"Edges\")\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "7OQRMLI-ziRz",
        "outputId": "bc24b580-6cb7-4e7c-d60d-102c6c47a24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We ran all models against all unprocessed ontology graphs for embedding sizes [20,50,100] to study the general perofrmance of traditional KGE methods. Since we did not include the embedding size as a hyperparameter in the inner loop (we tested each on test) we are not allowed to choose for each method the best model over all embeddings sizes, so we randomaly choose 50 to plot performance. This turns out to be okay because the relative performance of the methods is largely the same regardless of the dimension sizes chosen."
      ],
      "metadata": {
        "id": "ESEYrBx0IGVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_1 = pd.read_csv(onto_path + \"/results_1.csv\")\n",
        "df = results_1\n",
        "filtered_df = df[(df['hidden_dims'] == 50)]\n",
        "selected_columns_df = filtered_df[['dataset', 'model', 'test_mean_rank', 'test_hits_at_10']]\n",
        "selected_columns_df.sort_values(by = \"dataset\", inplace = True)\n",
        "df_2 = selected_columns_df.set_index(\"dataset\")\n",
        "df_2[\"test_mean_rank\"] = [int(x) for x in df_2[\"test_mean_rank\"]]\n",
        "df_2.pivot(columns = \"model\", values = \"test_mean_rank\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "NNrcNglh7yk1",
        "outputId": "81ca619a-1bc9-4292-8339-6ae57f349a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-325-e6fb4588ba92>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  selected_columns_df.sort_values(by = \"dataset\", inplace = True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model    complexE  distM  transE  transR\n",
              "dataset                                 \n",
              "cl           3318   3273     740     610\n",
              "go           8679   8437    4808    4142\n",
              "hp           6757   6725    4812    5895\n",
              "uberon       3542   3594     767     831"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-418592f5-aa5e-41b5-b560-28c577201053\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>model</th>\n",
              "      <th>complexE</th>\n",
              "      <th>distM</th>\n",
              "      <th>transE</th>\n",
              "      <th>transR</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cl</th>\n",
              "      <td>3318</td>\n",
              "      <td>3273</td>\n",
              "      <td>740</td>\n",
              "      <td>610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>go</th>\n",
              "      <td>8679</td>\n",
              "      <td>8437</td>\n",
              "      <td>4808</td>\n",
              "      <td>4142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hp</th>\n",
              "      <td>6757</td>\n",
              "      <td>6725</td>\n",
              "      <td>4812</td>\n",
              "      <td>5895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uberon</th>\n",
              "      <td>3542</td>\n",
              "      <td>3594</td>\n",
              "      <td>767</td>\n",
              "      <td>831</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-418592f5-aa5e-41b5-b560-28c577201053')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-418592f5-aa5e-41b5-b560-28c577201053 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-418592f5-aa5e-41b5-b560-28c577201053');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 325
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically the same code but with some adjustement to process the results of running transE and transR on the \"_trans\" datasets. We accidentally also run an iteration of complexE which is filtered out."
      ],
      "metadata": {
        "id": "Gr3KrDobJmJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_2 = pd.read_csv(onto_path + \"/results_2.csv\")\n",
        "df = results_2\n",
        "filtered_df = df[(df['hidden_dims'] == 50) & (df['model'] != \"complexE\")]\n",
        "selected_columns_df = filtered_df[['dataset', 'model', 'test_mean_rank', 'test_hits_at_10']]\n",
        "selected_columns_df.sort_values(by = \"dataset\", inplace = True)\n",
        "df_2 = selected_columns_df.set_index(\"dataset\")\n",
        "df_2[\"test_mean_rank\"] = [int(x) for x in df_2[\"test_mean_rank\"]]\n",
        "df_2 = df_2.loc[['cl_only_trans', 'cl_trans', 'go_only_trans', 'go_trans',\n",
        "       'hp_only_trans', 'hp_trans', 'uberon_only_trans',\n",
        "       'uberon_trans']]\n",
        "df_2.pivot(columns = \"model\", values = \"test_mean_rank\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "5tuIN9ELs__4",
        "outputId": "390efeda-6284-4bea-a754-e0bc83762c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-323-b03c574efa15>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  selected_columns_df.sort_values(by = \"dataset\", inplace = True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model              transE  transR\n",
              "dataset                          \n",
              "cl_only_trans          70      30\n",
              "cl_trans              131     237\n",
              "go_only_trans         301      84\n",
              "go_trans              422     194\n",
              "hp_only_trans          70      34\n",
              "hp_trans              123      42\n",
              "uberon_only_trans     119      71\n",
              "uberon_trans          152     178"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39fdbb9b-efb8-4820-b5da-ca86a9fab1c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>model</th>\n",
              "      <th>transE</th>\n",
              "      <th>transR</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cl_only_trans</th>\n",
              "      <td>70</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cl_trans</th>\n",
              "      <td>131</td>\n",
              "      <td>237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>go_only_trans</th>\n",
              "      <td>301</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>go_trans</th>\n",
              "      <td>422</td>\n",
              "      <td>194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hp_only_trans</th>\n",
              "      <td>70</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hp_trans</th>\n",
              "      <td>123</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uberon_only_trans</th>\n",
              "      <td>119</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uberon_trans</th>\n",
              "      <td>152</td>\n",
              "      <td>178</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39fdbb9b-efb8-4820-b5da-ca86a9fab1c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39fdbb9b-efb8-4820-b5da-ca86a9fab1c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39fdbb9b-efb8-4820-b5da-ca86a9fab1c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting function used to plot the mini graphs of the uberon ontology which we used to experiment with subsetE in order to understand its training behaviour (shown in Medium post)."
      ],
      "metadata": {
        "id": "tKFnYbgUKaoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=200, node_color='lightblue')\n",
        "nx.draw_networkx_labels(G, pos, font_size=16, font_weight='bold')\n",
        "\n",
        "nx.draw_networkx_edges(G, pos, arrows=True, arrowstyle='-|>', arrowsize=20, edge_color='black')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6mU37HVKJuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting functions used to plot the embeddings obtained from running subsetE on these small graphs. For these examples we used 3 points in 2D space, i.e., flat triangles."
      ],
      "metadata": {
        "id": "LMK_g9VkKkt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "points = expand_last_dimension(model.node_emb.weight, 2).cpu()\n",
        "cmap = plt.cm.get_cmap('viridis', len(points))\n",
        "fig, ax = plt.subplots()\n",
        "for i, set_points in enumerate(points):\n",
        "\n",
        "    x = set_points[:, 0].detach().numpy()\n",
        "    y = set_points[:, 1].detach().numpy()\n",
        "    color = cmap(i)\n",
        "    ax.scatter(x, y, color=color)\n",
        "    \n",
        "    ax.plot(x[[0, 1]], y[[0, 1]], linestyle='-', color=color)\n",
        "    ax.plot(x[[1, 2]], y[[1, 2]], linestyle='-', color=color)\n",
        "    ax.plot(x[[2, 0]], y[[2, 0]], linestyle='-', color=color)\n",
        "    \n",
        "    ax.text(x.mean(), y.mean(), f\"{i}\", fontsize=8, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "ax.set_xlabel('X-axis')\n",
        "ax.set_ylabel('Y-axis')\n",
        "ax.set_title('Points in 2D Space')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IrqVrkolxuBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}